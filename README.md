# ![CondAmbigQA](./img/label.svg)

ğŸ  [Homepage](https://github.com/innovation64/CondAmbigQA) | ğŸ“ [Paper](https://arxiv.org/abs/2502.01523) | ğŸ¤— [Dataset](https://huggingface.co/datasets/Apocalypse-AGI-DAO/CondAmbigQA) | ğŸ†• [CondAmbigQA-2K](https://huggingface.co/datasets/Apocalypse-AGI-DAO/CondAmbigQA-2K) | ğŸ“Š [Results](#results) | ğŸŒ [ä¸­æ–‡](./README_zh.md)

ğŸ”„ Supported by RAG, LLMs, Conditions-based QA

ğŸ’« A novel benchmark for resolving ambiguous questions through conditions identification.

## ğŸ“Œ CondAmbigQA: A Conditional Ambiguous Question Answering Benchmark

- ğŸ“š News
  - ğŸ†• **CondAmbigQA-2K Dataset**: [Hugging Face](https://huggingface.co/datasets/Apocalypse-AGI-DAO/CondAmbigQA-2K) - Extended 2,000 sample dataset
  - ğŸ‰ Original Dataset: [Hugging Face](https://huggingface.co/datasets/Apocalypse-AGI-DAO/CondAmbigQA)
  - ğŸ“„ Paper Release: [arXiv](https://arxiv.org/abs/2502.01523)

- ğŸš€ Getting Started
  - [Installation](#installation)
  - [Dataset Usage](#dataset-usage)
  - [Model Evaluation](#evaluation)
  - [Annotation System](#annotation-system)
  - [Experiment Code](#experiment-code)

## ğŸ’¡ Quick Start

### Basic Dataset Usage

```python
from datasets import load_dataset
dataset = load_dataset("Apocalypse-AGI-DAO/CondAmbigQA")
```

### Run Experiments

```bash
# Run main experiment with retrieval
cd experiment_code/core
python main_experiment.py

# Run annotation system
cd annotation-system
python main.py --input data/questions.json --output data/annotated.json
```

## ğŸ—ï¸ Project Components

### ğŸ“ Directory Structure

```
CondAmbigQA/
â”œâ”€â”€ annotation-system/     # AI-AI annotation system
â”œâ”€â”€ experiment_code/       # Experimental evaluation code
â”œâ”€â”€ retrieval/            # Document retrieval components
â”œâ”€â”€ label/               # Labeling utilities
â””â”€â”€ results/             # Experimental results and visualizations
```

### ğŸ“ Annotation Methodology

**Primary Approach**: **Human-AI Collaborative Annotation** (as described in the paper)
- Expert human annotators working with AI assistance
- Manual quality control and validation
- Iterative refinement through human judgment

**Supplementary Tool**: **AI-AI Pre-Annotation System**
- **âš ï¸ Auxiliary tool only** - requires human verification
- Automated preliminary annotation generation
- 8-dimensional quality assessment for flagging issues
- Enhanced multi-agent collaboration with GPT-4o
- **Workflow**: AI-AI Pre-annotation â†’ **Human Expert Review** â†’ Final Annotation

### ğŸ§ª Experiment Code
- **Multiple Model Support**: OpenAI API and local Ollama models
- **Comprehensive Evaluation**: Multi-dimensional scoring system
- **Rich Visualization**: Statistical analysis and comparison charts
- **Flexible Configuration**: Easy model and parameter adjustment

## ğŸ“‹ Installation

### Prerequisites
- Python 3.7+
- OpenAI API key (for GPT models)
- Ollama (for local models)

### Setup
```bash
# Clone the repository
git clone https://github.com/innovation64/CondAmbigQA.git
cd CondAmbigQA

# Install main dependencies
pip install -r requirements.txt

# For annotation system
cd annotation-system
pip install -r requirements.txt

# For experiment code
cd experiment_code
pip install -r requirements.txt
```

### Environment Configuration
```bash
# For OpenAI models
export OPENAI_API_KEY="your_api_key"

# Ensure Ollama is running (for local models)
# Default: localhost:11434
```

## ğŸ“– Dataset Usage

### Loading the Dataset

#### Original CondAmbigQA Dataset
```python
from datasets import load_dataset
dataset = load_dataset("Apocalypse-AGI-DAO/CondAmbigQA")
```

#### Extended CondAmbigQA-2K Dataset (2,000 samples)
```python
from datasets import load_dataset
dataset_2k = load_dataset("Apocalypse-AGI-DAO/CondAmbigQA-2K")
```

### Data Structure
```json
{
  "id": "unique_id",
  "question": "Ambiguous question",
  "properties": [
    {
      "condition": "Condition description",
      "groundtruth": "Answer for condition",
      "condition_citations": [...],
      "answer_citations": [...]
    }
  ],
  "ctxs": [{"title": "...", "text": "..."}]
}
```

## ğŸ”¬ Model Evaluation

### Supported Models
- **OpenAI**: GPT-4o, GLM-4-plus
- **Local Ollama**: Llama3.1:8b, Qwen2.5, Mistral, Gemma2, GLM4, DeepSeek-R1

### Running Experiments
```bash
# Main experiment (with retrieval)
cd experiment_code/core
python main_experiment.py

# Pure LLM experiment (no retrieval)
python pure_llm_experiment.py

# Ground truth annotation
python ground_truth_experiment.py
```

### Visualization
```bash
cd experiment_code/visualization
python main_visualizer.py          # Comprehensive analysis
python comparison_visualizer.py    # Experiment comparison
python statistics_visualizer.py    # Statistical analysis
```

## ğŸ“Š Results & Performance Analysis

### ğŸ† Main Experiment Results (With Retrieval Augmentation)

| Model | Type | Condition Score | Answer Score | Citation Score | Combined Score | Rank |
|-------|------|----------------|-------------|----------------|----------------|------|
| **GPT-4o** | API | **0.552 Â± 0.190** | **0.558 Â± 0.157** | **0.875 Â± 0.207** | **0.662** | ğŸ¥‡ #1 |
| **GLM4-plus** | API | 0.302 Â± 0.069 | 0.420 Â± 0.097 | 0.441 Â± 0.261 | 0.388 | ğŸ¥ˆ #2 |
| **Qwen2.5** | Local | 0.235 Â± 0.120 | 0.287 Â± 0.161 | 0.558 Â± 0.359 | 0.360 | ğŸ¥‰ #3 |
| **DeepSeek-R1** | Local | 0.245 Â± 0.112 | 0.293 Â± 0.142 | 0.501 Â± 0.342 | 0.346 | #4 |
| GLM4 | Local | 0.231 Â± 0.071 | 0.290 Â± 0.090 | 0.320 Â± 0.215 | 0.280 | #5 |
| LLaMA3.1 | Local | 0.232 Â± 0.076 | 0.252 Â± 0.093 | 0.306 Â± 0.246 | 0.264 | #6 |
| Mistral | Local | 0.196 Â± 0.060 | 0.231 Â± 0.079 | 0.263 Â± 0.214 | 0.230 | #7 |
| Gemma2 | Local | 0.170 Â± 0.091 | 0.203 Â± 0.118 | 0.217 Â± 0.277 | 0.197 | #8 |
| **Average** | - | **0.270** | **0.317** | **0.435** | **0.341** | - |

### ğŸ“ˆ Comprehensive Performance Analysis

| Model Type | Pure LLM (Avg. Answer Score) | w/ Model-generated Conditions | w/ GT Conditions | Improvement |
|------------|------------------------------|-------------------------------|------------------|-------------|
| **Proprietary Models** | | | | |
| **GPT-4o** | **0.25** | **0.56** | **0.57** | **+128%** |
| GLM4-plus | 0.24 | 0.42 | 0.53 | +121% |
| **Local Models** | | | | |
| Qwen2.5 | 0.15 | 0.29 | 0.40 | +167% |
| Mistral | 0.17 | 0.23 | 0.29 | +61% |
| Gemma2 | 0.15 | 0.20 | 0.29 | +93% |
| LLaMA3.1 | 0.14 | 0.25 | 0.29 | +107% |
| GLM4 | 0.14 | 0.29 | 0.38 | +171% |
| **DeepSeek-R1** | 0.07 | 0.29 | 0.34 | **+400%** |
| **Average** | **0.164** | **0.316** | **0.386** | **+135%** |

### ğŸ¯ Key Performance Insights

#### ğŸš€ Main Experiment (RAG) Performance
- **Clear Winner**: **GPT-4o** dominates all metrics with 0.662 combined score
  - **Citation Excellence**: 0.875 citation score (far ahead of others)
  - **Balanced Performance**: Strong across conditions (0.552), answers (0.558), and citations
  
- **API vs Local Model Gap**: 
  - **API Models**: GPT-4o (0.662), GLM4-plus (0.388)
  - **Best Local**: Qwen2.5 (0.360) - competitive with API models
  - **Local Model Range**: 0.197 (Gemma2) to 0.360 (Qwen2.5)

- **Citation Performance Hierarchy**:
  - **Tier 1**: GPT-4o (0.875) - exceptional citation quality
  - **Tier 2**: Qwen2.5 (0.558), DeepSeek-R1 (0.501) - good citation skills
  - **Tier 3**: Others (0.217-0.441) - moderate citation ability

#### ğŸ“Š Cross-Experiment Analysis
- **Dramatic Improvement with Conditions**: All models show substantial performance gains when provided with conditions
  - **DeepSeek-R1**: Most dramatic improvement (+400%, from 0.07 to 0.34 in pure LLM vs conditional)
  - **Average Improvement**: +135% across all models
  - **Local Models**: Show higher improvement rates than proprietary models

- **Performance Consistency**:
  - **Most Stable**: GLM4-plus (Â±0.069 condition score variance)
  - **Most Variable**: GPT-4o (Â±0.190) but still top performer
  - **Standard Deviations**: Show model reliability across different questions

### ğŸ“Š Visualizations & Charts

The project includes comprehensive performance analysis charts organized by experiment type:

#### ğŸ¯ Main Experiment Visualizations
- **Comprehensive Performance**: `experiment_code/sample_charts/main_visualizations/comprehensive_performance.pdf`
- **Model Ranking**: `experiment_code/sample_charts/main_visualizations/model_ranking.pdf`
- **Score Distributions**: `experiment_code/sample_charts/main_visualizations/score_distributions.pdf`
- **Enhanced Model Stats Table**: `experiment_code/sample_charts/main_visualizations/enhanced_model_stats_table.pdf`

#### ğŸ”„ Comparison Analysis Charts  
- **Experiment Comparison**: `experiment_code/sample_charts/comparison_visualizations/experiment_comparison.pdf`
- **Cross-Experiment Performance**: `experiment_code/sample_charts/comparison_visualizations/comprehensive_performance.pdf`
- **Model Ranking Comparison**: `experiment_code/sample_charts/comparison_visualizations/model_ranking.pdf`

#### ğŸ“ˆ Statistical Analysis
- **Condition vs Answer Scatter Plot**: `experiment_code/sample_charts/statistical_visualizations/condition_vs_answer_scatter copy.pdf`

#### ğŸ“‹ Summary Charts (Root Level)
- **Overall Performance Summary**: `experiment_code/sample_charts/comprehensive_performance.pdf`
- **Model Ranking Overview**: `experiment_code/sample_charts/model_ranking.pdf`
- **Score Distribution Summary**: `experiment_code/sample_charts/score_distributions.pdf`
- **Enhanced Stats Table**: `experiment_code/sample_charts/enhanced_model_stats_table.pdf`

### ğŸ”¬ Experiment Types

1. **Main Experiments**: Full RAG pipeline with retrieval augmentation
2. **Pure LLM**: Models without external knowledge retrieval
3. **Ground Truth**: Human-annotated reference performance
4. **Comparison**: Cross-experiment performance analysis

## ğŸ“– Citation

```bibtex
@article{li2024condambigqa,
  title={CondAmbigQA: A Benchmark and Dataset for Conditional Ambiguous Question Answering},
  author={Li, Zongxi and Li, Yang and Xie, Haoran and Qin, S. Joe},
  journal={arXiv preprint arXiv:2502.01523},
  year={2025}
}
```

## ğŸ“¬ Contact 

- Email: zongxili@ln.edu.hk
