# CondAmbigQA - Clean Experimental Code

## ğŸ¯ Project Overview

CondAmbigQA (Conditional Ambiguous Question Answering) is a research project evaluating large language models' performance on ambiguous questions that require additional conditional information for accurate answering.

## ğŸ“ Directory Structure

```
clean_experiment_code/
â”œâ”€â”€ core/                      # Core experiment scripts
â”‚   â”œâ”€â”€ config.py             # Model configuration
â”‚   â”œâ”€â”€ models.py             # Model interface classes  
â”‚   â”œâ”€â”€ utils.py              # Utility functions
â”‚   â”œâ”€â”€ evaluator.py          # Evaluation framework
â”‚   â”œâ”€â”€ main_experiment.py    # Main experiment runner
â”‚   â”œâ”€â”€ main_and_comapre_experiment.py # Combined main & comparison experiments
â”‚   â”œâ”€â”€ pure_llm_experiment.py # Pure LLM experiment (no retrieval)
â”‚   â”œâ”€â”€ ground_truth_experiment.py # Ground truth annotation
â”‚   â””â”€â”€ scaling_other_dataset_experiment.py # Scaling to other datasets
â”œâ”€â”€ visualization/            # Visualization scripts  
â”‚   â”œâ”€â”€ main_visualizer.py    # Comprehensive result visualization
â”‚   â”œâ”€â”€ comparison_visualizer.py # Experiment comparison charts
â”‚   â”œâ”€â”€ statistics_visualizer.py # Statistical analysis plots
â”‚   â”œâ”€â”€ main_experiment_stats_viz.py # Main experiment statistics visualization
â”‚   â””â”€â”€ comp_experiment_stats_viz.py # Comparison experiment statistics visualization
â”œâ”€â”€ data/                     # Datasets
â”‚   â”œâ”€â”€ cleaned_mcaqa.json   # Main dataset (2,200 samples) - USE THIS
â”‚   â”œâ”€â”€ condambigqa_2000.json # Alternative dataset (2,000 samples)
â”‚   â””â”€â”€ mcaqa.json          # Legacy dataset (200 samples) - IGNORE
â”œâ”€â”€ sample_results/          # Complete result files (45+ files)
â”‚   â”œâ”€â”€ main_experiments/        # Main results (7 models)
â”‚   â”œâ”€â”€ comparison_experiments/  # Comparison results (7 models)  
â”‚   â”œâ”€â”€ ground_truth_experiments/ # Ground truth results (7 models)
â”‚   â”œâ”€â”€ pure_llm_experiments/    # Pure LLM results (8 models)
â”‚   â”œâ”€â”€ intermediate_results/    # Processing data samples (4 files)
â”‚   â””â”€â”€ summary_scores/         # Aggregated score files (6 files)
â”œâ”€â”€ retrieval/               # Retrieval augmentation
â”‚   â”œâ”€â”€ indexing.py          # Document indexing
â”‚   â””â”€â”€ retrieval_data.py    # Retrieval processing
â”œâ”€â”€ docs/                    # Documentation
â”‚   â”œâ”€â”€ DATA_REQUIREMENTS.md  # Detailed data requirements guide
â”‚   â””â”€â”€ COMPLETE_FILE_INVENTORY.md # Complete file listing (97 files)
â”œâ”€â”€ sample_charts/             # Complete visualization outputs (26 charts)
â”‚   â”œâ”€â”€ main_visualizations/       # Main experiment charts
â”‚   â”œâ”€â”€ comparison_visualizations/ # Comparison charts  
â”‚   â”œâ”€â”€ performance_analysis/      # Performance analysis
â”‚   â””â”€â”€ statistical_visualizations/ # Statistical charts
â””â”€â”€ requirements.txt         # Python dependencies
```

## ğŸ“Š Data Requirements

### Essential Dataset
- **Main Dataset**: `data/cleaned_mcaqa.json` (2,200 samples)
- **âš ï¸ IMPORTANT**: Do NOT use `mcaqa.json` (only 200 samples - outdated)

### Experiment â†’ Visualization Data Flow
1. **Run Experiments** â†’ Generates result files in current directory
2. **Move Results** â†’ Copy to `sample_results/` for visualization
3. **Run Visualization** â†’ Reads from `sample_results/`

**For Complete Data Requirements**: See `docs/DATA_REQUIREMENTS.md`

## ğŸš€ Quick Start

### 1. Environment Setup
```bash
pip install -r requirements.txt

# For OpenAI models
export OPENAI_API_KEY="your_api_key"

# For Ollama local models - ensure Ollama is running at localhost:11434
```

### 2. Run Experiments

#### Main Experiment (with retrieval augmentation)
```bash
cd core
python main_experiment.py
```

#### Pure LLM Experiment (no retrieval)
```bash
cd core  
python pure_llm_experiment.py
```

#### Ground Truth Annotation
```bash
cd core
python ground_truth_experiment.py
```

#### Combined Main & Comparison Experiment
```bash
cd core  
python main_and_comapre_experiment.py
```

#### Scaling to Other Datasets
```bash
cd core
python scaling_other_dataset_experiment.py
```

### 3. Visualization

**âš ï¸ Prerequisites**: Run experiments first to generate result files

#### Comprehensive Results Analysis
```bash
cd visualization
python main_visualizer.py
# Needs: results_*_main.json files in sample_results/
```

#### Compare Different Experiments  
```bash
cd visualization
python comparison_visualizer.py
# Needs: *_ground_truth.json, *_main.json, *_compare.json files
```

#### Statistical Analysis
```bash
cd visualization
python statistics_visualizer.py
# Works with: any results_*.json files
```

#### Main Experiment Statistics Visualization
```bash
cd visualization
python main_experiment_stats_viz.py
# Generates detailed statistical visualizations for main experiments
```

#### Comparison Experiment Statistics Visualization
```bash
cd visualization
python comp_experiment_stats_viz.py
# Generates statistical comparisons between different experiment types
```

## âš™ï¸ Configuration

### Supported Models (core/config.py)

**OpenAI Models:**
- `gpt-4o`
- `glm-4-plus`

**Local Ollama Models:**  
- `llama3.1:8b`
- `qwen2.5:latest`
- `mistral:latest`
- `gemma2:latest`
- `glm4:latest`
- `deepseek-r1:7b`

## ğŸ“Š Evaluation Metrics

- **Condition Score**: Accuracy of generated conditions
- **Answer Score**: Correctness of conditional answers  
- **Citation Score**: Quality of source citations
- **Overall Score**: Weighted combination

## ğŸ“ˆ Output Files

**Main Results:**
- `results_{model_name}.json` - Complete experiment results
- `intermediate_results_{model_name}_main_experiment.json` - Intermediate data

**Pure LLM Results:**  
- `pure_llm_results_{model_name}.json` - No retrieval results

**Ground Truth:**
- `results_{model_name}_ground_truth.json` - Annotated ground truth

## ğŸ› ï¸ Troubleshooting

### Common Issues

1. **API Connection Errors**
   - Check API keys and network connectivity
   - Verify Ollama service status for local models

2. **Memory Issues** 
   - Use smaller models or reduce batch size in config

3. **Missing Dependencies**
   - Run `pip install -r requirements.txt --force-reinstall`

## ğŸ“ Key Features

- **Clean Architecture**: Organized, maintainable code structure
- **Multiple Model Support**: OpenAI API and local Ollama models
- **Comprehensive Evaluation**: Multi-dimensional scoring system
- **Rich Visualization**: Statistical analysis and comparison charts  
- **Flexible Configuration**: Easy model and parameter adjustment

## ğŸ“„ License

Academic research use only.

---

## ğŸ”„ Workflow

1. **Setup** â†’ Install dependencies and configure models
2. **Run** â†’ Execute experiments using core scripts
3. **Analyze** â†’ Generate visualizations and statistics  
4. **Compare** â†’ Evaluate different models and approaches

For detailed Chinese documentation, see `README_CN.md`.